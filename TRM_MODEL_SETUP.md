# TRM Model Setup Guide

This guide explains how to get the TRM (Tiny Recursive Model) working in AtlasApp.

## Current Status

‚úÖ **Code**: Fully implemented and production-ready  
‚ùå **Models**: Missing CoreML model files  
‚úÖ **Workaround**: Mock engine available for testing

---

## Solution 1: Use Mock Engine (IMMEDIATE - 2 minutes)

**Best for:** Testing the app UI, voice features, and integrations without AI

### Steps:

1. **The mock engine is already created** at `Sources/Atlas/Services/TRMEngine/MockTRMEngine.swift`

2. **Use it in your code:**

```swift
// In any ViewModel or Service that needs TRM:

import Foundation

class ChatViewModel: ObservableObject {
    private let trmEngine: InferenceEngineProtocol
    
    init() {
        // Automatically uses mock if real models not available
        self.trmEngine = TRMEngineFactory.createEngine()
        
        // Or explicitly use mock:
        // self.trmEngine = TRMEngineFactory.createMockEngine()
    }
    
    func sendMessage(_ text: String) async {
        do {
            let response = try await trmEngine.generate(prompt: text, context: nil)
            // Handle response
        } catch {
            print("Error: \(error)")
        }
    }
}
```

3. **Build and run** - App will work with simulated AI responses

### What You Get:
- ‚úÖ App compiles and runs
- ‚úÖ Voice input/output works
- ‚úÖ UI fully functional
- ‚úÖ Contextual mock responses
- ‚ö†Ô∏è Not real AI (placeholder responses)

---

## Solution 2: Download Pre-Converted Models (RECOMMENDED - 30 minutes)

**Best for:** Getting real AI working quickly

### Option A: Use Phi-3.5-mini (Recommended)

Apple provides optimized CoreML models:

```bash
# 1. Download Phi-3.5-mini CoreML model
# Visit: https://huggingface.co/apple/phi-3.5-mini-coreml

# 2. Download these files:
# - phi-3.5-mini-4bit.mlpackage (recommended, ~2GB)
# - OR phi-3.5-mini-fp16.mlpackage (larger, ~7GB)

# 3. Rename for AtlasApp:
mv phi-3.5-mini-4bit.mlpackage ThinkModel_4bit.mlpackage
cp ThinkModel_4bit.mlpackage ActModel_4bit.mlpackage

# 4. Add to Xcode:
# - Open AtlasApp.xcodeproj
# - Drag .mlpackage files into project
# - Check "Copy items if needed"
# - Add to Atlas target
# - Build and run!
```

### Option B: Use Llama 3.2 1B

```bash
# 1. Download from Hugging Face
# Visit: https://huggingface.co/apple/llama-3.2-1b-coreml

# 2. Follow same steps as Phi-3.5
```

### What You Get:
- ‚úÖ Real AI inference
- ‚úÖ 30+ tokens/second on iPhone
- ‚úÖ Runs on Neural Engine
- ‚úÖ Full privacy (on-device)
- ‚ö†Ô∏è Not TRM architecture (but works!)

---

## Solution 3: Convert Samsung TRM Models (ADVANCED - 2-3 hours)

**Best for:** Using the actual TRM architecture

### Prerequisites:

```bash
# Install dependencies
pip install torch torchvision coremltools numpy transformers

# Or use conda:
conda create -n trm python=3.10
conda activate trm
pip install torch coremltools numpy
```

### Steps:

#### 1. Get Samsung's TRM Checkpoint

```bash
cd /Users/aniksahai/Desktop/claude-flow/TRMModel

# Clone Samsung's TRM repository
git clone https://github.com/SamsungLabs/TinyRecursiveModels.git tmp/TinyRecursiveModels

# Download pretrained checkpoint
# (Check their releases page for latest)
wget https://github.com/SamsungLabs/TinyRecursiveModels/releases/download/v1.0/trm_7m.pt
```

#### 2. Convert to CoreML

```bash
# Run conversion script
python convert_trm_to_coreml.py \
    --checkpoint trm_7m.pt \
    --output ../AtlasApp \
    --quantize 4bit

# This creates:
# - ThinkModel_4bit.mlpackage
# - ActModel_4bit.mlpackage
```

#### 3. Add to Xcode

```bash
# Open Xcode
open ../AtlasApp/Package.swift

# In Xcode:
# 1. File ‚Üí Add Files to "Atlas"
# 2. Select ThinkModel_4bit.mlpackage and ActModel_4bit.mlpackage
# 3. Check "Copy items if needed"
# 4. Add to Atlas target
# 5. Build (‚åò+B)
```

#### 4. Verify Models

```swift
// In your app, check if models loaded:
let engine = try TRMInferenceEngine()
let response = try await engine.generate(prompt: "Hello", context: nil)
print(response) // Should get real AI response!
```

### What You Get:
- ‚úÖ True TRM architecture
- ‚úÖ Recursive think-act cycles
- ‚úÖ 7M parameters (~2MB quantized)
- ‚úÖ Fastest inference (30+ tokens/sec)
- ‚úÖ Best for on-device

---

## Solution 4: Use Alternative Local Models

### Option: GGML/llama.cpp Integration

If TRM is hard to get, use llama.cpp:

```bash
# 1. Add llama.cpp Swift package
# In Package.swift:
.package(url: "https://github.com/ggerganov/llama.cpp", from: "1.0.0")

# 2. Download GGML model
# Visit: https://huggingface.co/TheBloke/phi-3.5-mini-GGUF
# Download: phi-3.5-mini-q4_0.gguf

# 3. Add to app bundle and use llama.cpp for inference
```

---

## Quick Comparison

| Solution | Time | AI Quality | Setup Difficulty |
|----------|------|------------|------------------|
| **Mock Engine** | 2 min | ‚ùå Fake | ‚≠ê Easiest |
| **Phi-3.5 CoreML** | 30 min | ‚úÖ Excellent | ‚≠ê‚≠ê Easy |
| **Llama 3.2 CoreML** | 30 min | ‚úÖ Excellent | ‚≠ê‚≠ê Easy |
| **Real TRM** | 2-3 hrs | ‚úÖ Good | ‚≠ê‚≠ê‚≠ê‚≠ê Advanced |
| **llama.cpp** | 1 hr | ‚úÖ Excellent | ‚≠ê‚≠ê‚≠ê Medium |

---

## Recommended Path

### For Testing (Right Now):
```swift
// Use mock engine
let engine = TRMEngineFactory.createMockEngine()
```

### For Production (This Week):
1. Download Phi-3.5-mini CoreML (30 min)
2. Rename to ThinkModel/ActModel
3. Add to Xcode
4. Ship it! ‚úÖ

### For True TRM (Later):
1. Get Samsung's checkpoint
2. Run conversion script
3. Replace Phi-3.5 with TRM
4. Enjoy recursive reasoning! üß†

---

## Testing Your Setup

```swift
// Test script
import Foundation

@main
struct TRMTest {
    static func main() async throws {
        print("üß™ Testing TRM Engine...")
        
        let engine = TRMEngineFactory.createEngine()
        
        let testPrompts = [
            "Hello, who are you?",
            "What is 2+2?",
            "Explain privacy in AI"
        ]
        
        for prompt in testPrompts {
            print("\nüìù Prompt: \(prompt)")
            let response = try await engine.generate(prompt: prompt, context: nil)
            print("ü§ñ Response: \(response)")
        }
        
        print("\n‚úÖ TRM Engine working!")
    }
}
```

---

## Troubleshooting

### "Model not found" error
- ‚úÖ Check files are in app bundle
- ‚úÖ Verify file names match exactly
- ‚úÖ Ensure files added to Atlas target
- ‚úÖ Clean build folder (‚åò+Shift+K)

### "Model failed to load" error
- ‚úÖ Check iOS version (need iOS 17+)
- ‚úÖ Verify .mlpackage format (not .mlmodel)
- ‚úÖ Try fp16 version if 4bit fails

### Slow inference
- ‚úÖ Enable Neural Engine in config
- ‚úÖ Use 4-bit quantized models
- ‚úÖ Check device isn't thermal throttling

---

## Next Steps

1. **Right now**: Use mock engine to test app
2. **This week**: Add Phi-3.5-mini for real AI
3. **Later**: Convert to real TRM if needed

Questions? Check the README or open an issue!

